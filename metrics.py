import torch
from torch import Tensor
import torchmetrics
import typing
from typing import Union
import transformers
import os
import torch.nn.functional as F
from tqdm import tqdm
from evo2.models import Evo2
import math

LOG2 = torch.log(torch.tensor(2.0))

class NLL(torchmetrics.aggregation.MeanMetric):
  pass

class BPD(NLL):
  def compute(self) -> Tensor:
    """Computes the bits per dimension.

    Returns:
      bpd
    """
    return self.mean_value / self.weight / LOG2

class Perplexity(NLL):
  def compute(self) -> Tensor:
    """Computes the Perplexity.

    Returns:
      Perplexity
    """
    return torch.exp(self.mean_value / self.weight)

class NFEs(torchmetrics.aggregation.MeanMetric):
  pass

class Metrics:
  def __init__(self, config=None) -> None:
    self.config=config
    metrics = torchmetrics.MetricCollection({
        'nll': NLL(), 'bpd': BPD(), 'ppl': Perplexity()})
    if hasattr(config, 'block_size'):
      self.block_size = config.block_size
    else:
      self.block_size = config.model.length
      
    self.nfes = NFEs()
    self.train_nlls = metrics.clone(prefix='train/')
    self.valid_nlls = metrics.clone(prefix='val/')
    self.gen_ppl = Perplexity()
    self.gen_entropy = NLL()
    self.gen_ppls, self.gen_nfes, self.gen_entropies, self.gen_lengths \
      = [], [], [], []

    self.sampling_eps = config.training.sampling_eps
    if getattr(config.algo, 'clip_search_delta', None):
      self.clip_search_delta = config.algo.clip_search_delta
    self.valid_vars = {self.sampling_eps: []}
    if getattr(config.algo, 'var_min', None):
      self.valid_vars = self.init_valid_vars()
    self.eval_ppl_batch_size = \
     self.config.eval.perplexity_batch_size
    self.gen_ppl_eval_model_name_or_path = \
      config.eval.gen_ppl_eval_model_name_or_path
    self.tokenizer = transformers.AutoTokenizer.\
      from_pretrained(self.gen_ppl_eval_model_name_or_path)
    if self.tokenizer.pad_token is None:
      self.tokenizer.pad_token = self.tokenizer.eos_token
      self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

  def init_valid_vars(self):
    eps = self.sampling_eps
    if self.block_size > 1:
      eps = self.sampling_eps
      self.valid_vars = {(eps, 1): []}
      for width in self.config.algo.clip_search_widths:
        for i in torch.arange(0, 1 - width + self.clip_search_delta, self.clip_search_delta):
          min = torch.clamp(i, min=self.sampling_eps).item()
          max = torch.clamp(i + width, min=self.sampling_eps).item()
          self.valid_vars[(min, max)] = []
    else:
      eps = self.sampling_eps
      self.valid_vars = {
        (eps, 1): [],
        (1, 1): []}


  def to(self, *args, **kwargs):
    self.train_nlls = self.train_nlls.to(*args, **kwargs)
    self.valid_nlls = self.valid_nlls.to(*args, **kwargs)
    self.gen_ppl = self.gen_ppl.to(*args, **kwargs)
    self.nfes = self.nfes.to(*args, **kwargs)
    self.gen_entropy = self.gen_entropy.to(*args, **kwargs)

  def reset(self):
    self.gen_ppls, self.gen_nfes, self.gen_entropies, self.gen_lengths \
      = [], [], [], []
    self.train_nlls.reset()
    self.valid_nlls.reset()
    self.gen_ppl.reset()
    self.gen_entropy.reset()
    self.nfes.reset()
    if getattr(self.config.algo, 'var_min', None):
      self.init_valid_vars()

  @torch.no_grad()
  def _eval_retokenize(self, text_samples, max_length,
                       device):
    """Retokenizes samples for the eval model.
    
    Args:
        text_samples: List of sentences generated by the model.
    Returns:
        samples: Samples re-tokenized for the eval model
        attn_mask: Attention mask for the eval model
        eval_context_size: Size of the context for the eval model
    """
    if 'llama2' in self.gen_ppl_eval_model_name_or_path:
      tokenizer_kwargs = {
        'text_samples': text_samples,
        'return_tensors': 'pt',
        'return_token_type_ids': False,
        'return_attention_mask': True,
        'truncation': True,
        'padding': True,
        'max_length': max_length,
      }
      eval_context_size = 4096
    else:
      tokenizer_kwargs = {
        'return_tensors': 'pt',
        'return_token_type_ids': False,
        'return_attention_mask': True,
        'truncation': True,
        'padding': True,
        'max_length': max_length,
      }
      eval_context_size = 1024
    samples = self.tokenizer(text_samples,
                             **tokenizer_kwargs)
    attn_mask = samples['attention_mask']
    samples = samples['input_ids']
    if 'llama2' not in self.gen_ppl_eval_model_name_or_path:
      attn_mask = attn_mask.to(device)
      samples = samples.to(device)      
    return samples, attn_mask, eval_context_size

  @torch.no_grad()
  def record_generative_perplexity(
    self,
    text_samples: typing.List[str],
    max_length: int,
    batch_size: Union[int, None] = None,
    retokenize: bool = True,
    stride=512,
    device='cuda') -> None:
    os.environ['TOKENIZERS_PARALLELISM'] = 'false'
    if 'arcinstitute/evo2' not in self.gen_ppl_eval_model_name_or_path:
        raise Exception('Reprogrammed perplexity reporting for evo')
    eval_model = Evo2(self.gen_ppl_eval_model_name_or_path.split('/')[1])
    ref_scores = eval_model.score_sequences(text_samples)
    accum = 0
    for score, idx in enumerate(ref_scores):
      self.gen_ppls.append(score)
      self.gen_ppl.update(accum, score)
      accum += score

      # record sample length
      self.gen_lengths.append(math.ceil(len(ref_scores[idx] / 6)))
